{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# I94 Immigration Data Warehouse\n",
    "### Data Engineering Capstone Project by Thelma Obirieze\n",
    "\n",
    "#### Project Summary\n",
    "The objective of this project is design a data warehouse for 194 Immigration data. This data is collected by the US National Tourism and Trade Office and contains details of all immigrants coming into the country and their ports of entry. \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import re\n",
    "import psycopg2\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from cleaning import clean_immigration_data, clean_city_data, clean_airport_data, clean_country_data\n",
    "from check import check_data\n",
    "from check import check_integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed, ran successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = \"/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df_spark =spark.read.load('./sas_data')\n",
    "print(\"Completed, ran successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(cicid=5748517.0, i94yr=2016.0, i94mon=4.0, i94cit=245.0, i94res=438.0, i94port='LOS', arrdate=20574.0, i94mode=1.0, i94addr='CA', depdate=20582.0, i94bir=40.0, i94visa=1.0, count=1.0, dtadfile='20160430', visapost='SYD', occup=None, entdepa='G', entdepd='O', entdepu=None, matflag='M', biryear=1976.0, dtaddto='10292016', gender='F', insnum=None, airline='QF', admnum=94953870030.0, fltno='00011', visatype='B1')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "In this project, I will be pulling the data from the I94 Immigration data store (sas_data) and I create fact and dimension tablesin the data warehouse where this data will be stored.\n",
    "\n",
    "In addition, I will be using the following data \n",
    "* US Cities Demographics data which contains data by US city, state, age, population, veteran status and race.\n",
    "* Countries Data\n",
    "* Airport codes data\n",
    "\n",
    "With the data, I plan to create a data warehouse that the Business Analysts can use to make further analysis. The team plans to analyze the number of immigrants that comes into US on a daily basis. They will like to know their source country, the airport they landed, the demography of the airport city they landed and the type of visa they came in with.\n",
    "\n",
    "\n",
    "#### Describe and Gather Data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Data Source 1: I94 immigration data\n",
    "\n",
    "The I94 immigration data is sourced from the US National Tourism and Trade Office and it contains the following structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "|5748518.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     NV|20591.0|  32.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1|\n",
      "|5748519.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20582.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1|\n",
      "|5748520.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     F|  null|     DL|9.495645143E10|00040|      B1|\n",
      "|5748521.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  28.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1988.0|10292016|     M|  null|     DL|9.495638813E10|00040|      B1|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in the data here\n",
    "df_immigration = df_spark\n",
    "df_immigration.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### I94 Immigration Data Structure\n",
    "\n",
    "Below shows the data structure of the Immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Data Source 2: US Cities Demographics data\n",
    "\n",
    "The US Cities Demographics data comes from OpenSoft. I will be using the csv format of the data for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "file_cities = './us-cities-demographics.csv'\n",
    "df_cities = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").load(file_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "|            City|         State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race| Count|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "|   Silver Spring|      Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino| 25924|\n",
      "|          Quincy| Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White| 58723|\n",
      "|          Hoover|       Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian|  4759|\n",
      "|Rancho Cucamonga|    California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...| 24437|\n",
      "|          Newark|    New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White| 76402|\n",
      "|          Peoria|      Illinois|      33.1|          56229|            62432|          118661|              6634|        7517|                   2.4|        IL|American Indian a...|  1343|\n",
      "|        Avondale|       Arizona|      29.1|          38712|            41971|           80683|              4815|        8355|                  3.18|        AZ|Black or African-...| 11592|\n",
      "|     West Covina|    California|      39.8|          51629|            56860|          108489|              3800|       37038|                  3.56|        CA|               Asian| 32716|\n",
      "|        O'Fallon|      Missouri|      36.0|          41762|            43270|           85032|              5783|        3269|                  2.77|        MO|  Hispanic or Latino|  2583|\n",
      "|      High Point|North Carolina|      35.5|          51751|            58077|          109828|              5204|       16315|                  2.65|        NC|               Asian| 11060|\n",
      "|          Folsom|    California|      40.9|          41051|            35317|           76368|              4187|       13234|                  2.62|        CA|  Hispanic or Latino|  5822|\n",
      "|          Folsom|    California|      40.9|          41051|            35317|           76368|              4187|       13234|                  2.62|        CA|American Indian a...|   998|\n",
      "|    Philadelphia|  Pennsylvania|      34.1|         741270|           826172|         1567442|             61995|      205339|                  2.61|        PA|               Asian|122721|\n",
      "|         Wichita|        Kansas|      34.6|         192354|           197601|          389955|             23978|       40270|                  2.56|        KS|  Hispanic or Latino| 65162|\n",
      "|         Wichita|        Kansas|      34.6|         192354|           197601|          389955|             23978|       40270|                  2.56|        KS|American Indian a...|  8791|\n",
      "|      Fort Myers|       Florida|      37.3|          36850|            37165|           74015|              4312|       15365|                  2.45|        FL|               White| 50169|\n",
      "|      Pittsburgh|  Pennsylvania|      32.9|         149690|           154695|          304385|             17728|       28187|                  2.13|        PA|               White|208863|\n",
      "|          Laredo|         Texas|      28.8|         124305|           131484|          255789|              4921|       68427|                  3.66|        TX|American Indian a...|  1253|\n",
      "|        Berkeley|    California|      32.5|          60142|            60829|          120971|              3736|       25000|                  2.35|        CA|               Asian| 27089|\n",
      "|     Santa Clara|    California|      35.2|          63278|            62938|          126216|              4426|       52281|                  2.75|        CA|               White| 55847|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cities.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### US Cities Data Structure\n",
    "\n",
    "Below shows the data structure of the us cities data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cities.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Data Source 3: Airport Codes Data\n",
    "In addition, I will be utilizing the airport codes file as a dimenion to show more information about the immigrants port of entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "file_ports = './airport-codes_csv.csv'\n",
    "df_airports = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_ports) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "| 00AS|small_airport|      Fulton Airport|        1100|       NA|         US|     US-OK|        Alex|    00AS|     null|      00AS|-97.8180194, 34.9...|\n",
      "| 00AZ|small_airport|      Cordes Airport|        3810|       NA|         US|     US-AZ|      Cordes|    00AZ|     null|      00AZ|-112.165000915527...|\n",
      "| 00CA|small_airport|Goldstone /Gts/ A...|        3038|       NA|         US|     US-CA|     Barstow|    00CA|     null|      00CA|-116.888000488, 3...|\n",
      "| 00CL|small_airport| Williams Ag Airport|          87|       NA|         US|     US-CA|       Biggs|    00CL|     null|      00CL|-121.763427, 39.4...|\n",
      "| 00CN|     heliport|Kitchen Creek Hel...|        3350|       NA|         US|     US-CA| Pine Valley|    00CN|     null|      00CN|-116.4597417, 32....|\n",
      "| 00CO|       closed|          Cass Field|        4830|       NA|         US|     US-CO|  Briggsdale|    null|     null|      null|-104.344002, 40.6...|\n",
      "| 00FA|small_airport| Grass Patch Airport|          53|       NA|         US|     US-FL|    Bushnell|    00FA|     null|      00FA|-82.2190017700195...|\n",
      "| 00FD|     heliport|  Ringhaver Heliport|          25|       NA|         US|     US-FL|   Riverview|    00FD|     null|      00FD|-82.3453979492187...|\n",
      "| 00FL|small_airport|   River Oak Airport|          35|       NA|         US|     US-FL|  Okeechobee|    00FL|     null|      00FL|-80.9692001342773...|\n",
      "| 00GA|small_airport|    Lt World Airport|         700|       NA|         US|     US-GA|    Lithonia|    00GA|     null|      00GA|-84.0682983398437...|\n",
      "| 00GE|     heliport|    Caffrey Heliport|         957|       NA|         US|     US-GA|       Hiram|    00GE|     null|      00GE|-84.7339019775390...|\n",
      "| 00HI|     heliport|  Kaupulehu Heliport|          43|       NA|         US|     US-HI| Kailua/Kona|    00HI|     null|      00HI|-155.980233, 19.8...|\n",
      "| 00ID|small_airport|Delta Shores Airport|        2064|       NA|         US|     US-ID|  Clark Fork|    00ID|     null|      00ID|-116.213996887207...|\n",
      "| 00IG|small_airport|       Goltl Airport|        3359|       NA|         US|     US-KS|    McDonald|    00IG|     null|      00IG|-101.395994, 39.7...|\n",
      "| 00II|     heliport|Bailey Generation...|         600|       NA|         US|     US-IN|  Chesterton|    00II|     null|      00II|-87.122802734375,...|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airports.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Airport codes Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_airports.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Data Source 4: Countries Codes Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_countries = spark.read.format(\"csv\").option(\"header\", \"true\").load('country-codes_csv.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FIFA: string (nullable = true)\n",
      " |-- Dial: string (nullable = true)\n",
      " |-- ISO3166-1-Alpha-3: string (nullable = true)\n",
      " |-- MARC: string (nullable = true)\n",
      " |-- is_independent: string (nullable = true)\n",
      " |-- ISO3166-1-numeric: string (nullable = true)\n",
      " |-- GAUL: string (nullable = true)\n",
      " |-- FIPS: string (nullable = true)\n",
      " |-- WMO: string (nullable = true)\n",
      " |-- ISO3166-1-Alpha-2: string (nullable = true)\n",
      " |-- ITU: string (nullable = true)\n",
      " |-- IOC: string (nullable = true)\n",
      " |-- DS: string (nullable = true)\n",
      " |-- UNTERM Spanish Formal: string (nullable = true)\n",
      " |-- Global Code: string (nullable = true)\n",
      " |-- Intermediate Region Code: string (nullable = true)\n",
      " |-- official_name_fr: string (nullable = true)\n",
      " |-- UNTERM French Short: string (nullable = true)\n",
      " |-- ISO4217-currency_name: string (nullable = true)\n",
      " |-- Developed / Developing Countries: string (nullable = true)\n",
      " |-- UNTERM Russian Formal: string (nullable = true)\n",
      " |-- UNTERM English Short: string (nullable = true)\n",
      " |-- ISO4217-currency_alphabetic_code: string (nullable = true)\n",
      " |-- Small Island Developing States (SIDS): string (nullable = true)\n",
      " |-- UNTERM Spanish Short: string (nullable = true)\n",
      " |-- ISO4217-currency_numeric_code: string (nullable = true)\n",
      " |-- UNTERM Chinese Formal: string (nullable = true)\n",
      " |-- UNTERM French Formal: string (nullable = true)\n",
      " |-- UNTERM Russian Short: string (nullable = true)\n",
      " |-- M49: string (nullable = true)\n",
      " |-- Sub-region Code: string (nullable = true)\n",
      " |-- Region Code: string (nullable = true)\n",
      " |-- official_name_ar: string (nullable = true)\n",
      " |-- ISO4217-currency_minor_unit: string (nullable = true)\n",
      " |-- UNTERM Arabic Formal: string (nullable = true)\n",
      " |-- UNTERM Chinese Short: string (nullable = true)\n",
      " |-- Land Locked Developing Countries (LLDC): string (nullable = true)\n",
      " |-- Intermediate Region Name: string (nullable = true)\n",
      " |-- official_name_es: string (nullable = true)\n",
      " |-- UNTERM English Formal: string (nullable = true)\n",
      " |-- official_name_cn: string (nullable = true)\n",
      " |-- official_name_en: string (nullable = true)\n",
      " |-- ISO4217-currency_country_name: string (nullable = true)\n",
      " |-- Least Developed Countries (LDC): string (nullable = true)\n",
      " |-- Region Name: string (nullable = true)\n",
      " |-- UNTERM Arabic Short: string (nullable = true)\n",
      " |-- Sub-region Name: string (nullable = true)\n",
      " |-- official_name_ru: string (nullable = true)\n",
      " |-- Global Name: string (nullable = true)\n",
      " |-- Capital: string (nullable = true)\n",
      " |-- Continent: string (nullable = true)\n",
      " |-- TLD: string (nullable = true)\n",
      " |-- Languages: string (nullable = true)\n",
      " |-- Geoname ID: string (nullable = true)\n",
      " |-- CLDR display name: string (nullable = true)\n",
      " |-- EDGAR: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_countries.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Identifying data quality issues in each of the data source\n",
    "\n",
    "In this steps I will be looking out for:\n",
    "* Duplicates\n",
    "* Nulls\n",
    "* Incorrect Data formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Cleaning Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_immigration2 = clean_immigration_data(spark, df_immigration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+------+------+---------+----------+---------+---------+--------+------+-------+--------+------------+--------------+\n",
      "|    cicid|i94yr|i94mon|i94cit|i94res|port_code|state_code|visa_type|mode_type|visapost|gender|airline|visatype|arrival_date|departure_date|\n",
      "+---------+-----+------+------+------+---------+----------+---------+---------+--------+------+-------+--------+------------+--------------+\n",
      "|5748517.0| 2016|     4|   245|   438|      LOS|        CA|        1|        1|     SYD|     F|     QF|      B1|  2016-04-30|    2016-05-08|\n",
      "|5748518.0| 2016|     4|   245|   438|      LOS|        NV|        1|        1|     SYD|     F|     VA|      B1|  2016-04-30|    2016-05-17|\n",
      "|5748519.0| 2016|     4|   245|   438|      LOS|        WA|        1|        1|     SYD|     M|     DL|      B1|  2016-04-30|    2016-05-08|\n",
      "|5748520.0| 2016|     4|   245|   438|      LOS|        WA|        1|        1|     SYD|     F|     DL|      B1|  2016-04-30|    2016-05-14|\n",
      "|5748521.0| 2016|     4|   245|   438|      LOS|        WA|        1|        1|     SYD|     M|     DL|      B1|  2016-04-30|    2016-05-14|\n",
      "+---------+-----+------+------+------+---------+----------+---------+---------+--------+------+-------+--------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: integer (nullable = true)\n",
      " |-- i94mon: integer (nullable = true)\n",
      " |-- i94cit: integer (nullable = true)\n",
      " |-- i94res: integer (nullable = true)\n",
      " |-- port_code: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- visa_type: integer (nullable = true)\n",
      " |-- mode_type: integer (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- arrival_date: date (nullable = true)\n",
      " |-- departure_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###### Cleaning US Cities data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Checking and droping duplicates \n",
    "Clean_df_cities = clean_city_data(spark, df_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|        State|median_age|male_population|female_population|total_population|foreign_born|average_household_size|state_code|                Race|Count|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy|Massachusetts|      41.0|          44129|            49500|           93629|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|      Alabama|      38.5|          38040|            46799|           84839|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|   California|      34.5|          88127|            87105|          175232|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|   New Jersey|      34.6|         138040|           143873|          281913|       86253|                  2.73|        NJ|               White|76402|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking for Nulls\n",
    "Clean_df_cities.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###### Cleaning airport data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# dropping records with nulls in iata_code column\n",
    "clean_df_airports = clean_airport_data(spark, df_airports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------+-----------+---------+----------+------------+--------+--------------------+----------+\n",
      "|iata_code|                name|          type|iso_country|continent|iso_region|municipality|gps_code|           port_city|port_state|\n",
      "+---------+--------------------+--------------+-----------+---------+----------+------------+--------+--------------------+----------+\n",
      "|      ALC|Alicante Internat...| large_airport|         ES|       EU|      ES-V|    Alicante|    LEAL|               ALCAN|        AK|\n",
      "|      ANC|Ted Stevens Ancho...| large_airport|         US|       NA|     US-AK|   Anchorage|    PANC|           ANCHORAGE|        AK|\n",
      "|      BAR|Qionghai Bo'ao Ai...|medium_airport|         CN|       AS|     CN-46|    Qionghai|    ZJQH|BAKER AAF - BAKER...|        AK|\n",
      "|      DAC|Hazrat Shahjalal ...| large_airport|         BD|       AS|      BD-3|       Dhaka|    VGHS|       DALTONS CACHE|        AK|\n",
      "|      PIZ|Point Lay LRRS Ai...|medium_airport|         US|       NA|     US-AK|   Point Lay|    PPIZ|DEW STATION PT LA...|        AK|\n",
      "+---------+--------------------+--------------+-----------+---------+----------+------------+--------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_df_airports.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "###### Cleaning Countries data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "Clean_df_countries = clean_country_data(spark, df_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+----------------+\n",
      "|code|          country|         Capital|\n",
      "+----+-----------------+----------------+\n",
      "|  TW|           Taiwan|          Taipei|\n",
      "|  AF|      Afghanistan|           Kabul|\n",
      "|  AL|          Albania|          Tirana|\n",
      "|  AG|          Algeria|         Algiers|\n",
      "|  AQ|   American Samoa|       Pago Pago|\n",
      "|  AN|          Andorra|Andorra la Vella|\n",
      "|  AO|           Angola|          Luanda|\n",
      "|  AV|         Anguilla|      The Valley|\n",
      "|  AY|       Antarctica|            null|\n",
      "|  AC|Antigua & Barbuda|      St. John's|\n",
      "|  AR|        Argentina|    Buenos Aires|\n",
      "|  AM|          Armenia|         Yerevan|\n",
      "|  AA|            Aruba|      Oranjestad|\n",
      "|  AS|        Australia|        Canberra|\n",
      "|  AU|          Austria|          Vienna|\n",
      "|  AJ|       Azerbaijan|            Baku|\n",
      "|  BF|          Bahamas|          Nassau|\n",
      "|  BA|          Bahrain|          Manama|\n",
      "|  BG|       Bangladesh|           Dhaka|\n",
      "|  BB|         Barbados|      Bridgetown|\n",
      "+----+-----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Clean_df_countries.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "For the purpose of this project, I will be using a star schema. I prefer start schema becuase it:\n",
    "\n",
    "* requires simpler queries/joins during reporting\n",
    "* has simplified business reporting logic\n",
    "* provides query performance gains\n",
    "\n",
    "The data model used for this project are:\n",
    "\n",
    "### Dimension Tables\n",
    "* dim_airports: This table is used to stored detailed information about each airports. This include: the name of the airport, the iata code, the city and country\n",
    "    iata_code \n",
    "    name        \n",
    "    type        \n",
    "    local_code  \n",
    "    coordinates  \n",
    "    city         \n",
    "    elevation_ft \n",
    "    continent    \n",
    "    iso_country\n",
    "    iso_region  \n",
    "    municipality\n",
    "    gps_code   \n",
    "    \n",
    "* dim_cities: This stores the demographic information about each of the US cities that immigrants will likely arrivate at. This include: the name of the city, their populations, the state etc.\n",
    "    city                  \n",
    "    state                 \n",
    "    media_age             \n",
    "    male_population        \n",
    "    female_population    \n",
    "    total_population     \n",
    "    num_veterans          \n",
    "    foreign_born         \n",
    "    average_household_size \n",
    "    state_code            \n",
    "    race                  \n",
    "    count \n",
    "    \n",
    "* dim_countries: This stores country codes and the full name of the counry and their capitals\n",
    "    country_code                   \n",
    "    country                \n",
    "    capital             \n",
    "\n",
    "### Fact table(s)\n",
    "* fact_immigration: This tsores the information about the immigrants that comes into US. This include the counry they departe from, the departure date, the arrivate date and city, the type of visa they are arriving to the US with etc.\n",
    "    cicid   \n",
    "    year    \n",
    "    month   \n",
    "    cit      \n",
    "    res     \n",
    "    port_code    \n",
    "    state_code  \n",
    "    visa_type    \n",
    "    mode_type   \n",
    "    visapost  \n",
    "    gender    \n",
    "    airline   \n",
    "    visatype    \n",
    "    arrival_date \n",
    "    departure_date \n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "\n",
    "The steps involves in the data pipeline process are:\n",
    "\n",
    "The data pipeline for this project eill be done with Apache Airflow. The data pipeline to take should be in this format\n",
    "\n",
    "##### Start process and read the data from sources\n",
    "\n",
    "* start_process >> read_immigration_data\n",
    "* start_process >> read_city_data\n",
    "* start_process >> read_airports_data\n",
    "* start_process >> read_countries_data\n",
    "\n",
    "\n",
    "##### Cleaning the data \n",
    "* read_immigration_data >> clean_immigration_data\n",
    "* read_city_data >> clean_city_data\n",
    "* read_airports_data >> clean_airports_data\n",
    "* read_countries_data >> clean_countries_data\n",
    "\n",
    "\n",
    "##### transforming the data \n",
    "* clean_immigration_data >> transform_immigration_data\n",
    "* clean_city_data >> transform_city_data\n",
    "* clean_airports_data >> transform_airports_data\n",
    "* clean_countries_data >> transform_countries_data\n",
    "\n",
    "\n",
    "##### Loading the transformed data to the data warehouse\n",
    "* transform_immigration_data >> write_fact_immigration_parquet\n",
    "* transform_city_data >> write_dim_city_parquet\n",
    "* transform_airports_data >> write_dim_airports_parquet\n",
    "* transform_countries_data >> write_dim_countries_parquet\n",
    "\n",
    "\n",
    "##### Ending the process \n",
    "* write_dim_city_parquet >> end_process\n",
    "* write_dim_airports_parquet >> end_process\n",
    "* write_dim_countries_parquet >> end_process\n",
    "* write_fact_immigration_parquet >> end_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "Clean_df_cities.write.parquet(\"dim_city\", mode='overwrite')\n",
    "clean_df_airports.write.parquet(\"dim_airport\", mode='overwrite')\n",
    "Clean_df_countries.write.parquet(\"dim_country\", mode='overwrite')\n",
    "df_immigration2.write.parquet(\"fact_immigrations\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "For data quality checks, \n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    2891|\n",
      "+--------+\n",
      "\n",
      "None\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     557|\n",
      "+--------+\n",
      "\n",
      "None\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     250|\n",
      "+--------+\n",
      "\n",
      "None\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 3096313|\n",
      "+--------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# make sure each row has the expected row numbers : 2891 expected for dim_city\n",
    "check_data(spark, 'dim_city')\n",
    "\n",
    "# make sure each row has the expected row numbers : 557 expected for dim_airport\n",
    "check_data(spark, 'dim_airport')\n",
    "\n",
    "# make sure each row has the expected row numbers : 250 expected for dim_country\n",
    "check_data(spark, 'dim_country')\n",
    "\n",
    "# make sure each row has the expected row numbers : 3096313 expected for fact_immigrations\n",
    "\n",
    "check_data(spark, 'fact_immigrations')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# checking that all fact columns joined with the dimension tables have correct values.\n",
    "dim_city = spark.read.parquet(\"dim_city/\")\n",
    "dim_airport = spark.read.parquet(\"dim_airport/\")\n",
    "dim_country = spark.read.parquet(\"dim_country/\")\n",
    "fact_immigrations = spark.read.parquet(\"fact_immigrations/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|state_code|\n",
      "+----------+\n",
      "|        MD|\n",
      "|        MA|\n",
      "|        AL|\n",
      "|        CA|\n",
      "|        NJ|\n",
      "|        IL|\n",
      "|        AZ|\n",
      "|        CA|\n",
      "|        MO|\n",
      "|        NC|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------+\n",
      "|state_code|\n",
      "+----------+\n",
      "|        CA|\n",
      "|        NV|\n",
      "|        WA|\n",
      "|        WA|\n",
      "|        WA|\n",
      "|        HI|\n",
      "|        HI|\n",
      "|        HI|\n",
      "|        FL|\n",
      "|        CA|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## State code relationship between dim_city and fact_immigrations\n",
    "\n",
    "dim_city.select('state_code').show(10)\n",
    "fact_immigrations.select('state_code').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Below shows that the state_code field in fact_immigrations and state field in dim_city has the same type data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|iso_country|\n",
      "+-----------+\n",
      "|         ES|\n",
      "|         US|\n",
      "|         CN|\n",
      "|         BD|\n",
      "|         US|\n",
      "|         US|\n",
      "|         ET|\n",
      "|         AU|\n",
      "|         US|\n",
      "|         IN|\n",
      "+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----+\n",
      "|code|\n",
      "+----+\n",
      "|  TW|\n",
      "|  AF|\n",
      "|  AL|\n",
      "|  AG|\n",
      "|  AQ|\n",
      "|  AN|\n",
      "|  AO|\n",
      "|  AV|\n",
      "|  AY|\n",
      "|  AC|\n",
      "+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# country code relationship between dim_airport and dim_country\n",
    "dim_airport.select(\"iso_country\").show(10)\n",
    "dim_country.select(\"code\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "This show that they two has the same kind of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|iata_code|\n",
      "+---------+\n",
      "|      ALC|\n",
      "|      ANC|\n",
      "|      BAR|\n",
      "|      DAC|\n",
      "|      PIZ|\n",
      "|      DTH|\n",
      "|      EGL|\n",
      "|      FRB|\n",
      "|      HOM|\n",
      "|      HYD|\n",
      "+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---------+\n",
      "|port_code|\n",
      "+---------+\n",
      "|      LOS|\n",
      "|      LOS|\n",
      "|      LOS|\n",
      "|      LOS|\n",
      "|      LOS|\n",
      "|      HHW|\n",
      "|      HHW|\n",
      "|      HHW|\n",
      "|      HOU|\n",
      "|      LOS|\n",
      "+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# port_code relationship between dim_airport and fact_immigrations\n",
    "dim_airport.select(\"iata_code\").show(10)\n",
    "fact_immigrations.select('port_code').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The two has the same kind of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "\n",
    "### Airports \n",
    "\n",
    "* IDENT - Identification code\n",
    "* TYPE - Type of the airport\n",
    "* NAME - Name of the Airport\n",
    "* ELEVATION_FT - Elevation above the sea level in feet\n",
    "* CONTINENT - Continent code\n",
    "* ISO_COUNTRY - Country code according to ISO **This has have a foreign key relationship with code in dim_country table\n",
    "* ISO_REGION - Region code according to ISO\n",
    "* MUNICIPALITY - Mucipality where the airport is located\n",
    "* GPS_CODE - GPS code\n",
    "* IATA_CODE - Code of the airport assigned by International Air Transport Association  **Primary Key\n",
    "* LOCAL_CODE - Local code of the airport\n",
    "* COORDINATES - GPS coordinates - longitude and latitude  \n",
    "\n",
    "### Cities\n",
    "* STATE_CODE - Two-letter code of the state **Primary Key\n",
    "* STATE - Name of the state\n",
    "* MEDIAN_AGE - Median age in the state (estimation)\n",
    "* AVERAGE_HOUSEHOLD_SIZE - Average number of people loving in a household in the state (estimation)\n",
    "* TOTAL_POPULATION - Number of citizens\n",
    "* FEMALE_POPULATION - Number of female citizens\n",
    "* MALE_POPULATION - Number of male citizens\n",
    "* NUMBER_OF_VETERANS - Number of veteran citizens\n",
    "* BLACK_OR_AFRICAN_AMERICAN - Number of citizens belonging to this ethnic group\n",
    "* HISPANIC_OR_LATINO - Number of citizens belonging to this ethnic group\n",
    "* ASIAN - Number of citizens belonging to this ethnic group\n",
    "* AMERICAN_INDIAN_AND_ALASKA_NATIVE - Number of citizens belonging to this ethnic group\n",
    "* WHITE - Number of citizens belonging to this ethnic group\n",
    "* FOREIGN_BORN - Number of citizens born outside of US \n",
    "    \n",
    "### Countries\n",
    "* COUNTRY_CODE   - Country Code **Primary Key           \n",
    "* COUNTRY      - Country name          \n",
    "* CAPITAL     - country's capital        \n",
    "\n",
    "### I94 Immigration \n",
    "* CICID - Record ID\n",
    "* I94YR - 4 digit year\n",
    "* I94MON - Numeric month\n",
    "* I94CIT - Contry of citizenship\n",
    "* I94RES - Country of residence  \n",
    "* I94PORT - Airport of addmittance into the USA  **This has forign key relationship with dim_airport\n",
    "* ARRDATE - Arrival date in the USA\n",
    "* I94MODE - Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported)\n",
    "* I94ADDR - State of arrival **This has foreign key relationship with state_code in the dim_city table\n",
    "* DEPDATE - Departure date\n",
    "* I94BIR - Age of the visitor\n",
    "* I94VISA - Visa codes: (1 = Business; 2 = Pleasure; 3 = Student)\n",
    "* DTADFILE - Character date field\n",
    "* GENDER - Gender of the visitor\n",
    "* VISAPOST - Department of State where where Visa was issued\n",
    "* FLTNO - Flight number of Airline used to arrive in U.S.\n",
    "* VISATYPE - Class of admission legally admitting the non-immigrant to temporarily stay in U.S.\n",
    "* arrival_year - Numeric year of the arrival (used for data partitioning)\n",
    "* arrival_month - Numeric month of the arrival (used for data partitioning)\n",
    "* arrival_day - Numeric day of the arrival (used for data partitioning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project. I tried running this project with pandas dataframe but because of the volume of data in the I94 Immigration data file, it took a very long time to process. I decided to use Apache Spark becuase it was faster to process\n",
    "* Propose how often the data should be updated and why. I plan to update the data once daily. I choose this approach because that will help me load only finalized dataset into the data warehouse. This is also becuase the Analyst team are not interested in analyszing current day's data. \n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x. If I am faced with a scenario where the data increases by 100x, I would consider Scaling the whole pipeline horizontally by adding new nodes or moving Spark to cluster mode using a cluster manager such as a Yarn\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day. This shouldnt be a challenge as the entire process runs in less than 20mins currently.\n",
    " * The database needed to be accessed by 100+ people. Once the data is ready to be consumed, it would be stored in a redshift cluster postgres database on that easily supports multiuser access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
